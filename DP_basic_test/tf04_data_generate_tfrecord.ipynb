{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
      "matplotlib 3.2.1\n",
      "numpy 1.18.5\n",
      "pandas 1.0.4\n",
      "sklearn 0.23.1\n",
      "tensorflow 2.2.0\n",
      "tensorflow.keras 2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_00.csv  test_08.csv   train_06.csv  train_14.csv  valid_02.csv\r\n",
      "test_01.csv  test_09.csv   train_07.csv  train_15.csv  valid_03.csv\r\n",
      "test_02.csv  train_00.csv  train_08.csv  train_16.csv  valid_04.csv\r\n",
      "test_03.csv  train_01.csv  train_09.csv  train_17.csv  valid_05.csv\r\n",
      "test_04.csv  train_02.csv  train_10.csv  train_18.csv  valid_06.csv\r\n",
      "test_05.csv  train_03.csv  train_11.csv  train_19.csv  valid_07.csv\r\n",
      "test_06.csv  train_04.csv  train_12.csv  valid_00.csv  valid_08.csv\r\n",
      "test_07.csv  train_05.csv  train_13.csv  valid_01.csv  valid_09.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls generate_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./generate_csv/train_08.csv',\n",
      " './generate_csv/train_11.csv',\n",
      " './generate_csv/train_18.csv',\n",
      " './generate_csv/train_15.csv',\n",
      " './generate_csv/train_17.csv',\n",
      " './generate_csv/train_00.csv',\n",
      " './generate_csv/train_01.csv',\n",
      " './generate_csv/train_19.csv',\n",
      " './generate_csv/train_14.csv',\n",
      " './generate_csv/train_02.csv',\n",
      " './generate_csv/train_16.csv',\n",
      " './generate_csv/train_09.csv',\n",
      " './generate_csv/train_03.csv',\n",
      " './generate_csv/train_12.csv',\n",
      " './generate_csv/train_10.csv',\n",
      " './generate_csv/train_13.csv',\n",
      " './generate_csv/train_05.csv',\n",
      " './generate_csv/train_07.csv',\n",
      " './generate_csv/train_04.csv',\n",
      " './generate_csv/train_06.csv']\n",
      "['./generate_csv/valid_01.csv',\n",
      " './generate_csv/valid_05.csv',\n",
      " './generate_csv/valid_02.csv',\n",
      " './generate_csv/valid_04.csv',\n",
      " './generate_csv/valid_08.csv',\n",
      " './generate_csv/valid_07.csv',\n",
      " './generate_csv/valid_06.csv',\n",
      " './generate_csv/valid_00.csv',\n",
      " './generate_csv/valid_09.csv',\n",
      " './generate_csv/valid_03.csv']\n",
      "['./generate_csv/test_00.csv',\n",
      " './generate_csv/test_07.csv',\n",
      " './generate_csv/test_01.csv',\n",
      " './generate_csv/test_08.csv',\n",
      " './generate_csv/test_06.csv',\n",
      " './generate_csv/test_02.csv',\n",
      " './generate_csv/test_04.csv',\n",
      " './generate_csv/test_05.csv',\n",
      " './generate_csv/test_09.csv',\n",
      " './generate_csv/test_03.csv']\n"
     ]
    }
   ],
   "source": [
    "source_dir = \"./generate_csv/\"\n",
    "\n",
    "#通过判断开头去添加文件\n",
    "def get_filenames_by_prefix(source_dir, prefix_name):\n",
    "    all_files = os.listdir(source_dir)\n",
    "    results = []\n",
    "    for filename in all_files:\n",
    "        if filename.startswith(prefix_name):\n",
    "            results.append(os.path.join(source_dir, filename))\n",
    "    return results\n",
    "\n",
    "train_filenames = get_filenames_by_prefix(source_dir, \"train\")\n",
    "valid_filenames = get_filenames_by_prefix(source_dir, \"valid\")\n",
    "test_filenames = get_filenames_by_prefix(source_dir, \"test\")\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(train_filenames)\n",
    "pprint.pprint(valid_filenames)\n",
    "pprint.pprint(test_filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下面的接口都是之前用过的\n",
    "def parse_csv_line(line, n_fields = 9):\n",
    "    defs = [tf.constant(np.nan)] * n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1:])\n",
    "    return x, y\n",
    "\n",
    "def csv_reader_dataset(filenames, n_readers=5,\n",
    "                       batch_size=32, n_parse_threads=5,\n",
    "                       shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length = n_readers\n",
    "    )\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_csv_line,\n",
    "                          num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 32\n",
    "train_set = csv_reader_dataset(train_filenames,\n",
    "                               batch_size = batch_size)\n",
    "valid_set = csv_reader_dataset(valid_filenames,\n",
    "                               batch_size = batch_size)\n",
    "test_set = csv_reader_dataset(test_filenames,\n",
    "                              batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter_4.tar.gz\t   tf02_data_generate_csv.ipynb\r\n",
      "generate_csv\t\t   tf03-tfrecord_basic_api.ipynb\r\n",
      "temp.csv\t\t   tf04_data_generate_tfrecord.ipynb\r\n",
      "tf01-data_basic_api.ipynb  tfrecord_basic\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把基础的如何序列化的步骤搞到一个函数\n",
    "def serialize_example(x, y):\n",
    "    \"\"\"Converts x, y to tf.train.Example and serialize\"\"\"\n",
    "    input_feautres = tf.train.FloatList(value = x)\n",
    "    label = tf.train.FloatList(value = y)\n",
    "    features = tf.train.Features(\n",
    "        feature = {\n",
    "            \"input_features\": tf.train.Feature(\n",
    "                float_list = input_feautres),\n",
    "            \"label\": tf.train.Feature(float_list = label)\n",
    "        }\n",
    "    )\n",
    "    #把features变为example\n",
    "    example = tf.train.Example(features = features)\n",
    "    return example.SerializeToString()  #把example序列化\n",
    "#n_shards是存为多少个文件，steps_per_shard和 steps_per_epoch类似\n",
    "def csv_dataset_to_tfrecords(base_filename, dataset,\n",
    "                             n_shards, steps_per_shard,\n",
    "                             compression_type = None):\n",
    "    #压缩文件类型\n",
    "    options = tf.io.TFRecordOptions(\n",
    "        compression_type = compression_type)\n",
    "    all_filenames = []\n",
    "    \n",
    "    for shard_id in range(n_shards):\n",
    "        filename_fullpath = '{}_{:05d}-of-{:05d}'.format(\n",
    "            base_filename, shard_id, n_shards)\n",
    "        #打开文件\n",
    "        with tf.io.TFRecordWriter(filename_fullpath, options) as writer:\n",
    "            #取出数据,为什么skip，上一个文件写了前500行，下一个文件存后面的数据\n",
    "            for x_batch, y_batch in dataset.skip(shard_id * steps_per_shard).take(steps_per_shard):\n",
    "                for x_example, y_example in zip(x_batch, y_batch):\n",
    "                    writer.write(\n",
    "                        serialize_example(x_example, y_example))\n",
    "        all_filenames.append(filename_fullpath)\n",
    "    #返回所有tfrecord文件名\n",
    "    return all_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf generate_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 8), dtype=float32, numpy=\n",
      "array([[ 4.2408210e-01,  9.1296333e-01, -4.4374820e-02, -1.5297213e-01,\n",
      "        -2.4727628e-01, -1.0539167e-01,  8.6126745e-01, -1.3357790e+00],\n",
      "       [ 4.0127665e-01, -9.2934215e-01, -5.3330503e-02, -1.8659453e-01,\n",
      "         6.5456617e-01,  2.6434466e-02,  9.3125278e-01, -1.4406418e+00],\n",
      "       [-1.1199750e+00, -1.3298433e+00,  1.4190045e-01,  4.6581370e-01,\n",
      "        -1.0301778e-01, -1.0744184e-01, -7.9505241e-01,  1.5304717e+00],\n",
      "       [ 4.9710345e-02, -8.4924191e-01, -6.2146995e-02,  1.7878747e-01,\n",
      "        -8.0253541e-01,  5.0660671e-04,  6.4664572e-01, -1.1060793e+00],\n",
      "       [ 6.3034356e-01,  1.8741661e+00, -6.7132145e-02, -1.2543367e-01,\n",
      "        -1.9737554e-01, -2.2722632e-02, -6.9240725e-01,  7.2652334e-01],\n",
      "       [ 4.3692350e-01, -1.9706452e+00, -1.6642106e-01,  5.4862052e-02,\n",
      "        -8.3791959e-01, -1.3239880e-01, -9.9567705e-01,  9.4124246e-01],\n",
      "       [-8.7577540e-01,  1.8741661e+00, -9.4874996e-01, -9.6571848e-02,\n",
      "        -7.1634322e-01, -7.7901915e-02,  9.8257536e-01, -1.4206679e+00],\n",
      "       [-9.4909388e-01,  6.7266262e-01,  2.8370556e-01,  1.0655530e-01,\n",
      "        -6.5464777e-01, -6.2394928e-02,  2.1273656e-01,  2.4704977e-03],\n",
      "       [-1.4538510e+00,  1.8741661e+00, -1.1315714e+00,  3.6112761e-01,\n",
      "        -3.9788580e-01, -3.2738592e-02, -7.3906416e-01,  6.4662784e-01],\n",
      "       [ 1.6312258e+00,  3.5226166e-01,  4.0805761e-02, -1.4088951e-01,\n",
      "        -4.6321040e-01, -6.7516237e-02, -8.2771224e-01,  5.9669316e-01],\n",
      "       [-4.9303812e-01, -1.5701441e+00, -6.9338977e-01,  1.6277646e-01,\n",
      "         3.2794318e-01, -8.8065289e-02, -8.6503774e-01,  6.3664091e-01],\n",
      "       [ 7.7511555e-01,  1.8741661e+00,  1.5645972e-01, -1.8905191e-01,\n",
      "        -6.2924379e-01, -8.7916031e-02, -7.4839550e-01,  5.7172585e-01],\n",
      "       [ 3.8743126e+00, -8.4924191e-01,  1.2254810e+00, -2.3587925e-02,\n",
      "         1.0202891e-01,  3.3357147e-02, -1.2289616e+00,  1.1709419e+00],\n",
      "       [-1.1179502e+00,  3.5226166e-01, -1.7415480e-01,  1.0293573e-01,\n",
      "        -2.4364713e-01, -6.1952524e-02,  1.9063820e+00, -1.1210598e+00],\n",
      "       [ 4.0492255e-02, -6.8904144e-01, -4.4379851e-01,  2.2374585e-02,\n",
      "        -2.2187227e-01, -1.4828503e-01, -8.8836622e-01,  6.3664091e-01],\n",
      "       [ 5.9460921e+00,  8.3286309e-01,  1.5784487e+00, -1.5852584e-01,\n",
      "        -9.2048264e-01, -6.1406735e-02, -7.4372983e-01,  5.3677154e-01],\n",
      "       [-7.1208769e-01,  9.9306357e-01,  1.8163487e-02,  1.5576729e-01,\n",
      "        -5.4486614e-01,  7.7402845e-02,  9.8257536e-01, -1.2958312e+00],\n",
      "       [ 9.9832129e-01,  9.1296333e-01,  2.7784565e-01, -3.8538378e-01,\n",
      "        -8.8872761e-01, -4.5322746e-02, -6.8307585e-01,  5.7671928e-01],\n",
      "       [ 7.1197432e-01, -8.4924191e-01,  2.8004515e-01, -9.2923619e-02,\n",
      "        -2.6723656e-01,  1.6180091e-01, -1.3735980e+00,  1.2758048e+00],\n",
      "       [-4.2771223e-01, -9.2934215e-01, -3.5796812e-01,  6.9204621e-02,\n",
      "        -8.1523740e-01, -5.4645728e-02, -5.7109928e-01,  5.5175197e-01],\n",
      "       [ 1.3139210e+00, -9.2934215e-01, -2.8761879e-01, -1.3543741e-01,\n",
      "         2.0092310e-01, -1.6999629e-01,  1.0385636e+00, -1.4606156e+00],\n",
      "       [-6.2022644e-01, -1.0895426e+00, -3.7012932e-01,  1.3973571e-01,\n",
      "        -2.1279940e-01, -7.6531373e-02,  5.4400051e-01, -7.2431520e-02],\n",
      "       [-5.0763786e-01,  1.7139657e+00, -1.2830897e-01,  9.2301741e-02,\n",
      "         7.5527495e-01,  4.5815628e-02,  9.7324395e-01, -1.4206679e+00],\n",
      "       [-3.8135535e-01, -1.0094423e+00,  5.6857741e-01,  5.8230191e-01,\n",
      "        -1.1772447e+00,  5.6477685e-02,  1.2408846e-01, -2.1224862e-01],\n",
      "       [-1.0497243e-01, -2.1308458e+00, -8.5628080e-01,  3.7446755e-01,\n",
      "        -7.7259499e-01, -1.9822408e-01,  9.9657243e-01, -1.4256613e+00],\n",
      "       [ 7.3206228e-01, -1.0094423e+00,  9.3781338e+00,  5.9055953e+00,\n",
      "        -4.9859455e-01,  1.4240813e-02, -6.4108467e-01,  1.1909158e+00],\n",
      "       [ 1.4595456e+00, -1.4900438e+00, -1.5207647e-02, -4.1439840e-01,\n",
      "        -8.5878718e-01, -4.9030025e-02,  1.2345226e+00, -1.3407724e+00],\n",
      "       [-6.8443340e-01,  4.3236190e-01, -9.1752118e-01, -1.7607674e-01,\n",
      "         5.6383759e-01,  1.3709892e-01, -7.4372983e-01,  7.7645802e-01],\n",
      "       [-9.4573700e-01,  3.1860713e-02, -7.5199944e-01, -1.6781589e-01,\n",
      "         5.8198327e-01,  8.1732191e-02, -7.4839550e-01,  6.3164747e-01],\n",
      "       [-2.4814592e-01, -1.0895426e+00, -3.1784795e-02, -1.6791259e-01,\n",
      "        -9.6403235e-01, -6.5253571e-02,  1.4631414e+00, -1.5055568e+00],\n",
      "       [ 3.2819037e+00, -7.6914167e-01,  3.4618150e-02,  1.3071860e-01,\n",
      "        -5.9113777e-01, -1.7783472e-01, -7.8105533e-01,  5.5674541e-01],\n",
      "       [-9.2717163e-02,  5.9256238e-01, -1.3964359e-01, -5.7545114e-02,\n",
      "        -4.0786594e-01, -1.4770859e-01, -7.4839550e-01,  5.4675847e-01]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
      "array([[3.955  ],\n",
      "       [2.512  ],\n",
      "       [0.66   ],\n",
      "       [2.286  ],\n",
      "       [2.419  ],\n",
      "       [1.73   ],\n",
      "       [2.75   ],\n",
      "       [0.607  ],\n",
      "       [1.875  ],\n",
      "       [3.376  ],\n",
      "       [2.033  ],\n",
      "       [4.851  ],\n",
      "       [5.00001],\n",
      "       [0.603  ],\n",
      "       [2.852  ],\n",
      "       [5.00001],\n",
      "       [0.923  ],\n",
      "       [3.824  ],\n",
      "       [1.33   ],\n",
      "       [2.385  ],\n",
      "       [3.461  ],\n",
      "       [0.875  ],\n",
      "       [2.464  ],\n",
      "       [0.625  ],\n",
      "       [2.75   ],\n",
      "       [2.552  ],\n",
      "       [2.181  ],\n",
      "       [1.563  ],\n",
      "       [2.146  ],\n",
      "       [1.031  ],\n",
      "       [5.00001],\n",
      "       [5.00001]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for i in train_set.take(1):\n",
    "    print(i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35 s, sys: 9.33 s, total: 44.3 s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 训练集和测试集都分20\n",
    "n_shards = 20\n",
    "train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "valid_steps_per_shard = 3880 // batch_size // 10\n",
    "test_steps_per_shard = 5170 // batch_size // 10\n",
    "\n",
    "output_dir = \"generate_tfrecords\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "train_basename = os.path.join(output_dir, \"train\")\n",
    "valid_basename = os.path.join(output_dir, \"valid\")\n",
    "test_basename = os.path.join(output_dir, \"test\")\n",
    "\n",
    "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "    train_basename, train_set, n_shards, train_steps_per_shard, None)\n",
    "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "    valid_basename, valid_set, 10, valid_steps_per_shard, None)\n",
    "test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
    "    test_basename, test_set, 10, test_steps_per_shard, None)\n",
    "#执行会发现目录下总计生成了60个文件,这里文件数目改为一致，为了对比时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总用量 1960\r\n",
      "-rw-rw-r-- 1 luke luke 47616 May  3 11:29 test_00000-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 47616 May  3 11:29 test_00001-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 47616 May  3 11:29 test_00002-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 47616 May  3 11:29 test_00003-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 47616 May  3 11:29 test_00004-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 47616 May  3 11:29 test_00005-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 47616 May  3 11:29 test_00006-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 47616 May  3 11:29 test_00007-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 47616 May  3 11:29 test_00008-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 47616 May  3 11:29 test_00009-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00000-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00001-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00002-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00003-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00004-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00005-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00006-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00007-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00008-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00009-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00010-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00011-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00012-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00013-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00014-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00015-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00016-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00017-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00018-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 53568 May  3 11:29 train_00019-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 35712 May  3 11:29 valid_00000-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 35712 May  3 11:29 valid_00001-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 35712 May  3 11:29 valid_00002-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 35712 May  3 11:29 valid_00003-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 35712 May  3 11:29 valid_00004-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 35712 May  3 11:29 valid_00005-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 35712 May  3 11:29 valid_00006-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 35712 May  3 11:29 valid_00007-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 35712 May  3 11:29 valid_00008-of-00010\r\n",
      "-rw-rw-r-- 1 luke luke 35712 May  3 11:29 valid_00009-of-00010\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l generate_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成一下压缩的\n",
    "# n_shards = 20\n",
    "# train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "# valid_steps_per_shard = 3880 // batch_size // n_shards\n",
    "# test_steps_per_shard = 5170 // batch_size // n_shards\n",
    "\n",
    "# output_dir = \"generate_tfrecords_zip\"\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.mkdir(output_dir)\n",
    "\n",
    "# train_basename = os.path.join(output_dir, \"train\")\n",
    "# valid_basename = os.path.join(output_dir, \"valid\")\n",
    "# test_basename = os.path.join(output_dir, \"test\")\n",
    "# #只需修改参数的类型即可\n",
    "# train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "#     train_basename, train_set, n_shards, train_steps_per_shard,\n",
    "#     compression_type = \"GZIP\")\n",
    "# valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "#     valid_basename, valid_set, n_shards, valid_steps_per_shard,\n",
    "#     compression_type = \"GZIP\")\n",
    "# test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
    "#     test_basename, test_set, n_shards, test_steps_per_shard,\n",
    "#     compression_type = \"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总用量 860\r\n",
      "-rw-rw-r-- 1 luke luke 10171 May  7 11:16 test_00000-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10230 May  7 11:16 test_00001-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10204 May  7 11:16 test_00002-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10213 May  7 11:16 test_00003-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10229 May  7 11:16 test_00004-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10200 May  7 11:16 test_00005-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10199 May  7 11:16 test_00006-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10215 May  7 11:16 test_00007-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10179 May  7 11:16 test_00008-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10149 May  7 11:16 test_00009-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10141 May  7 11:16 test_00010-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10221 May  7 11:16 test_00011-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10209 May  7 11:16 test_00012-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10214 May  7 11:16 test_00013-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10212 May  7 11:16 test_00014-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10209 May  7 11:16 test_00015-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10185 May  7 11:16 test_00016-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10266 May  7 11:16 test_00017-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10258 May  7 11:16 test_00018-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 10170 May  7 11:16 test_00019-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22359 May  7 19:17 train_00000-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22447 May  7 19:17 train_00001-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22366 May  7 19:17 train_00002-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22311 May  7 19:17 train_00003-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22384 May  7 19:17 train_00004-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22341 May  7 19:17 train_00005-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22416 May  7 19:17 train_00006-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22285 May  7 19:17 train_00007-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22415 May  7 19:17 train_00008-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22365 May  7 19:17 train_00009-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22431 May  7 19:17 train_00010-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22367 May  7 19:17 train_00011-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22346 May  7 19:17 train_00012-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22332 May  7 19:17 train_00013-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22452 May  7 19:17 train_00014-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke    20 May  7 19:17 train_00015-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22427 May  7 11:16 train_00016-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22427 May  7 11:16 train_00017-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22454 May  7 11:16 train_00018-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke 22309 May  7 11:16 train_00019-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7747 May  7 11:16 valid_00000-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7744 May  7 11:16 valid_00001-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7749 May  7 11:16 valid_00002-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7755 May  7 11:16 valid_00003-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7744 May  7 11:16 valid_00004-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7678 May  7 11:16 valid_00005-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7762 May  7 11:16 valid_00006-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7720 May  7 11:16 valid_00007-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7727 May  7 11:16 valid_00008-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7739 May  7 11:16 valid_00009-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7762 May  7 11:16 valid_00010-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7727 May  7 11:16 valid_00011-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7729 May  7 11:16 valid_00012-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7763 May  7 11:16 valid_00013-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7727 May  7 11:16 valid_00014-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7749 May  7 11:16 valid_00015-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7741 May  7 11:16 valid_00016-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7753 May  7 11:16 valid_00017-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7702 May  7 11:16 valid_00018-of-00020\r\n",
      "-rw-rw-r-- 1 luke luke  7711 May  7 11:16 valid_00019-of-00020\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l generate_tfrecords_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_tfrecords/train_00000-of-00020',\n",
      " 'generate_tfrecords/train_00001-of-00020',\n",
      " 'generate_tfrecords/train_00002-of-00020',\n",
      " 'generate_tfrecords/train_00003-of-00020',\n",
      " 'generate_tfrecords/train_00004-of-00020',\n",
      " 'generate_tfrecords/train_00005-of-00020',\n",
      " 'generate_tfrecords/train_00006-of-00020',\n",
      " 'generate_tfrecords/train_00007-of-00020',\n",
      " 'generate_tfrecords/train_00008-of-00020',\n",
      " 'generate_tfrecords/train_00009-of-00020',\n",
      " 'generate_tfrecords/train_00010-of-00020',\n",
      " 'generate_tfrecords/train_00011-of-00020',\n",
      " 'generate_tfrecords/train_00012-of-00020',\n",
      " 'generate_tfrecords/train_00013-of-00020',\n",
      " 'generate_tfrecords/train_00014-of-00020',\n",
      " 'generate_tfrecords/train_00015-of-00020',\n",
      " 'generate_tfrecords/train_00016-of-00020',\n",
      " 'generate_tfrecords/train_00017-of-00020',\n",
      " 'generate_tfrecords/train_00018-of-00020',\n",
      " 'generate_tfrecords/train_00019-of-00020']\n",
      "['generate_tfrecords/valid_00000-of-00010',\n",
      " 'generate_tfrecords/valid_00001-of-00010',\n",
      " 'generate_tfrecords/valid_00002-of-00010',\n",
      " 'generate_tfrecords/valid_00003-of-00010',\n",
      " 'generate_tfrecords/valid_00004-of-00010',\n",
      " 'generate_tfrecords/valid_00005-of-00010',\n",
      " 'generate_tfrecords/valid_00006-of-00010',\n",
      " 'generate_tfrecords/valid_00007-of-00010',\n",
      " 'generate_tfrecords/valid_00008-of-00010',\n",
      " 'generate_tfrecords/valid_00009-of-00010']\n",
      "['generate_tfrecords/test_00000-of-00010',\n",
      " 'generate_tfrecords/test_00001-of-00010',\n",
      " 'generate_tfrecords/test_00002-of-00010',\n",
      " 'generate_tfrecords/test_00003-of-00010',\n",
      " 'generate_tfrecords/test_00004-of-00010',\n",
      " 'generate_tfrecords/test_00005-of-00010',\n",
      " 'generate_tfrecords/test_00006-of-00010',\n",
      " 'generate_tfrecords/test_00007-of-00010',\n",
      " 'generate_tfrecords/test_00008-of-00010',\n",
      " 'generate_tfrecords/test_00009-of-00010']\n"
     ]
    }
   ],
   "source": [
    "#打印一下文件名\n",
    "pprint.pprint(train_tfrecord_filenames)\n",
    "pprint.pprint(valid_tfrecord_filenames)\n",
    "pprint.pprint(test_tfrecord_fielnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 µs, sys: 0 ns, total: 36 µs\n",
      "Wall time: 41.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#把数据读取出来\n",
    "expected_features = {\n",
    "    \"input_features\": tf.io.FixedLenFeature([8], dtype=tf.float32),\n",
    "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.float32)\n",
    "}\n",
    "\n",
    "def parse_example(serialized_example):\n",
    "    example = tf.io.parse_single_example(serialized_example,\n",
    "                                         expected_features)\n",
    "    return example[\"input_features\"], example[\"label\"]\n",
    "\n",
    "def tfrecords_reader_dataset(filenames, n_readers=5,\n",
    "                             batch_size=32, n_parse_threads=5,\n",
    "                             shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "#         lambda filename: tf.data.TFRecordDataset(\n",
    "#             filename, compression_type = \"GZIP\"),\n",
    "          lambda filename: tf.data.TFRecordDataset(\n",
    "            filename),\n",
    "          cycle_length = n_readers\n",
    "    )\n",
    "    #洗牌，就是给数据打乱\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_example,\n",
    "                          num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)  #原来写进去是一条一条的sample，要分配\n",
    "    return dataset\n",
    "\n",
    "#测试一下，tfrecords_reader_dataset是否可以正常运行\n",
    "# tfrecords_train = tfrecords_reader_dataset(train_tfrecord_filenames,\n",
    "#                                            batch_size = 3)\n",
    "# for x_batch, y_batch in tfrecords_train.take(10):\n",
    "#     print(x_batch)\n",
    "#     print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f2844749620> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f2844749620>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f2844749620> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f2844749620>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function parse_example at 0x7f2844749730> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <function parse_example at 0x7f2844749730>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function parse_example at 0x7f2844749730> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <function parse_example at 0x7f2844749730>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f28447499d8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f28447499d8>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f28447499d8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f28447499d8>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f28447b6b70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f28447b6b70>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f28447b6b70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f28447b6b70>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "CPU times: user 197 ms, sys: 113 ms, total: 310 ms\n",
      "Wall time: 250 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#得到dataset,dataset是tensor，可以直接拿tensor训练\n",
    "\n",
    "batch_size = 32\n",
    "tfrecords_train_set = tfrecords_reader_dataset(\n",
    "    train_tfrecord_filenames, batch_size = batch_size)\n",
    "tfrecords_valid_set = tfrecords_reader_dataset(\n",
    "    valid_tfrecord_filenames, batch_size = batch_size)\n",
    "tfrecords_test_set = tfrecords_reader_dataset(\n",
    "    test_tfrecord_fielnames, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfrecords_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 8), dtype=float32, numpy=\n",
      "array([[ 4.85305160e-01, -8.49241912e-01, -6.53012618e-02,\n",
      "        -2.33796556e-02,  1.49743509e+00, -7.79065788e-02,\n",
      "        -9.02363241e-01,  7.81451464e-01],\n",
      "       [ 1.11519516e-01,  4.32361901e-01, -5.00933290e-01,\n",
      "        -3.21237385e-01, -1.15819156e+00,  2.72315852e-02,\n",
      "         7.86616385e-01, -1.16100752e+00],\n",
      "       [-6.97008371e-01, -6.89041436e-01,  3.55311275e-01,\n",
      "         3.80839527e-01, -7.09084928e-01, -6.55453503e-02,\n",
      "         1.58911502e+00, -1.64038050e+00],\n",
      "       [ 4.95588928e-01, -7.69141674e-01, -1.79629475e-01,\n",
      "        -1.61498353e-01, -4.26011652e-01, -3.83407474e-02,\n",
      "         9.59246933e-01, -1.42566133e+00],\n",
      "       [-3.21411103e-01,  8.32863092e-01,  1.33539941e-02,\n",
      "        -2.07708240e-01, -3.38912189e-01,  3.30382772e-02,\n",
      "        -8.23046565e-01,  6.56614780e-01],\n",
      "       [-1.11997497e+00, -1.32984328e+00,  1.41900450e-01,\n",
      "         4.65813696e-01, -1.03017777e-01, -1.07441843e-01,\n",
      "        -7.95052409e-01,  1.53047168e+00],\n",
      "       [ 3.71544302e-01, -2.05074549e+00,  1.40517309e-01,\n",
      "         5.87197244e-02,  1.61991870e+00, -7.16339946e-02,\n",
      "        -9.91011381e-01,  9.41242456e-01],\n",
      "       [-1.26165652e+00,  2.72161424e-01, -3.93901408e-01,\n",
      "        -6.56583458e-02,  2.86208004e-01,  1.46434650e-01,\n",
      "         3.24713111e-01,  1.32300660e-01],\n",
      "       [-6.69993520e-01, -2.08439991e-01, -1.48968905e-01,\n",
      "        -3.11829567e-01, -5.52124441e-01,  2.64506750e-02,\n",
      "         5.85991740e-01, -2.02261686e-01],\n",
      "       [-4.64584589e-01,  1.87416613e+00, -4.85015452e-01,\n",
      "        -1.04617871e-01, -5.06760120e-01, -7.24149048e-02,\n",
      "         9.91906762e-01, -1.35075927e+00],\n",
      "       [-1.07750773e+00, -4.48740691e-01, -5.68056822e-01,\n",
      "        -1.42692626e-01, -9.66667682e-02,  1.23264685e-01,\n",
      "        -3.14486384e-01, -4.81895894e-01],\n",
      "       [-1.00743961e+00,  1.71396565e+00, -2.46902421e-01,\n",
      "        -1.42927185e-01,  7.90659130e-01,  5.09991944e-02,\n",
      "        -7.67058253e-01,  6.26653969e-01],\n",
      "       [-9.28686202e-01, -1.24974310e+00, -9.14782062e-02,\n",
      "        -3.02846313e-01, -5.51217139e-01, -1.09450996e-01,\n",
      "         1.64976895e+00, -1.02618384e+00],\n",
      "       [-1.18961680e+00,  5.12462139e-01, -8.42191160e-01,\n",
      "        -5.92396893e-02, -2.90826023e-01, -1.59983411e-01,\n",
      "        -7.25067079e-01,  6.11673594e-01],\n",
      "       [-1.01351392e+00, -9.29342151e-01, -4.13469255e-01,\n",
      "         5.21443367e-01, -1.27514496e-01, -1.63647160e-03,\n",
      "        -1.14031339e+00,  1.09604001e+00],\n",
      "       [ 4.32630107e-02, -1.08954263e+00, -3.88787180e-01,\n",
      "        -1.07898645e-01, -6.81866348e-01, -7.23870993e-02,\n",
      "        -8.88366222e-01,  8.21399212e-01],\n",
      "       [ 1.11474633e+00, -3.68640482e-01,  6.53097093e-01,\n",
      "        -2.34616131e-01, -4.67746824e-01, -1.67696574e-03,\n",
      "        -7.06404328e-01,  9.61216331e-01],\n",
      "       [ 5.30276656e-01,  5.12462139e-01,  2.13416908e-02,\n",
      "        -2.20490396e-01, -3.96071225e-01,  1.56008918e-02,\n",
      "        -6.45750344e-01,  5.61738908e-01],\n",
      "       [-1.11006415e+00,  1.15326405e+00, -4.11490381e-01,\n",
      "         5.82470112e-02, -1.77328736e-02,  1.13085337e-01,\n",
      "         1.07122350e+00, -8.41425598e-01],\n",
      "       [ 1.89950967e+00, -2.08439991e-01,  1.06921792e+00,\n",
      "        -1.23476245e-01,  6.84593171e-02, -1.96188577e-02,\n",
      "        -9.16360319e-01,  8.16405773e-01],\n",
      "       [ 2.51504374e+00,  1.07316375e+00,  5.57440102e-01,\n",
      "        -1.72735125e-01, -6.12912595e-01, -1.90915652e-02,\n",
      "        -5.71099281e-01, -2.74903104e-02],\n",
      "       [-8.69807661e-01,  1.87416613e+00, -2.30041429e-01,\n",
      "        -1.29571825e-01, -7.18157828e-01, -1.24068499e-01,\n",
      "         1.66376603e+00, -7.41556227e-01],\n",
      "       [ 2.74194866e-01,  1.07316375e+00,  4.19374913e-01,\n",
      "        -9.63326171e-02, -4.99501824e-01, -3.76197398e-02,\n",
      "         1.16920292e+00, -8.91360283e-01],\n",
      "       [-8.95734206e-02,  7.52762854e-01, -4.88562912e-01,\n",
      "        -1.61846191e-01, -3.22581023e-01,  8.59187022e-02,\n",
      "        -6.31753266e-01,  5.71725845e-01],\n",
      "       [-9.61082757e-01, -4.82395217e-02, -9.69826826e-04,\n",
      "         1.55025929e-01, -8.07979167e-01, -6.15385585e-02,\n",
      "         2.27963710e+00, -1.41567433e+00],\n",
      "       [-7.43205428e-01,  9.12963331e-01, -6.44320250e-01,\n",
      "        -1.47909701e-01,  7.39851117e-01,  1.14276908e-01,\n",
      "        -7.95052409e-01,  6.81582153e-01],\n",
      "       [ 7.91100681e-01,  2.72161424e-01,  1.49001420e-01,\n",
      "        -1.30292848e-01, -4.05144066e-01, -7.35770464e-02,\n",
      "        -6.31753266e-01,  1.52274534e-01],\n",
      "       [ 9.69423540e-03, -1.00944233e+00, -6.23714626e-02,\n",
      "        -9.83398035e-02, -5.90230465e-01, -4.05768529e-02,\n",
      "         1.26718247e+00, -1.72027588e+00],\n",
      "       [-7.49736726e-02,  5.12462139e-01, -3.78685713e-01,\n",
      "         1.50380665e-02, -8.27032149e-01, -1.13242455e-01,\n",
      "         8.75264466e-01, -1.34576583e+00],\n",
      "       [-6.13406122e-01,  1.11960948e-01, -1.34190962e-01,\n",
      "         2.22381249e-01, -2.04547327e-02, -8.38030055e-02,\n",
      "        -7.81055331e-01,  1.09604001e+00],\n",
      "       [-9.49093878e-01,  6.72662616e-01,  2.83705562e-01,\n",
      "         1.06555298e-01, -6.54647768e-01, -6.23949282e-02,\n",
      "         2.12736562e-01,  2.47049774e-03],\n",
      "       [-9.63160813e-01,  6.72662616e-01, -2.42891103e-01,\n",
      "         2.25592270e-01, -4.69561398e-01, -7.39660338e-02,\n",
      "        -1.09196030e-01,  2.72117764e-01]], dtype=float32)>, <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
      "array([[2.956  ],\n",
      "       [2.     ],\n",
      "       [1.368  ],\n",
      "       [2.817  ],\n",
      "       [1.052  ],\n",
      "       [0.66   ],\n",
      "       [2.782  ],\n",
      "       [0.541  ],\n",
      "       [0.623  ],\n",
      "       [2.208  ],\n",
      "       [0.978  ],\n",
      "       [1.126  ],\n",
      "       [1.34   ],\n",
      "       [2.889  ],\n",
      "       [1.063  ],\n",
      "       [1.426  ],\n",
      "       [2.559  ],\n",
      "       [1.956  ],\n",
      "       [0.517  ],\n",
      "       [3.742  ],\n",
      "       [5.00001],\n",
      "       [1.039  ],\n",
      "       [1.857  ],\n",
      "       [1.546  ],\n",
      "       [0.72   ],\n",
      "       [1.438  ],\n",
      "       [3.852  ],\n",
      "       [2.688  ],\n",
      "       [3.397  ],\n",
      "       [1.5    ],\n",
      "       [0.607  ],\n",
      "       [0.532  ]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for i in tfrecords_train_set.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.8290 - val_loss: 1.4444\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 2.2042 - val_loss: 0.5141\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4487 - val_loss: 0.4691\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4077 - val_loss: 0.4416\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3934 - val_loss: 0.4212\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3769 - val_loss: 0.4152\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3744 - val_loss: 0.4132\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3556 - val_loss: 0.3951\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3580 - val_loss: 0.4157\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3458 - val_loss: 0.3849\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3464 - val_loss: 0.3807\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3466 - val_loss: 0.3761\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3404 - val_loss: 0.3733\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3388 - val_loss: 0.3739\n",
      "Epoch 15/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3240 - val_loss: 0.3657\n",
      "Epoch 16/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3395 - val_loss: 0.3583\n",
      "Epoch 17/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3345 - val_loss: 0.3572\n",
      "Epoch 18/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3207 - val_loss: 0.3579\n",
      "Epoch 19/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3265 - val_loss: 0.3595\n",
      "Epoch 20/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3267 - val_loss: 0.3541\n",
      "Epoch 21/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3226 - val_loss: 0.3572\n"
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu',\n",
    "                       input_shape=[8]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "    patience=5, min_delta=1e-2)]\n",
    "\n",
    "history = model.fit(tfrecords_train_set,\n",
    "                    validation_data = tfrecords_valid_set,\n",
    "                    steps_per_epoch = 11160 // batch_size,\n",
    "                    validation_steps = 3870 // batch_size,\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33755674958229065"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tfrecords_test_set, steps = 5160 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
